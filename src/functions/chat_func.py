import asyncio
import json
import logging
from typing import List, Tuple

import openai  # Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð±Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸
from openai import OpenAIError  # ÐžÐ±Ð½Ð¾Ð²Ð»Ñ‘Ð½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð´Ð»Ñ v1.0+ (Ð´ÐµÐºÐ°Ð±Ñ€ÑŒ 2025)
from telethon.events import NewMessage

from src.utils import LOG_PATH, model, max_token, sys_mess, read_existing_conversation, num_tokens_from_messages

Prompt = List[dict]

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚ OpenAI (Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð² 2025)
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def over_token(num_tokens: int, event: NewMessage, prompt: Prompt, filename: str):
    await event.reply(f"Ð Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¹ ({num_tokens} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²), Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽ Ð½Ð¾Ð²Ñ‹Ð¹! ðŸ˜…")
    prompt.append({"role": "user", "content": "ÐšÑ€Ð°Ñ‚ÐºÐ¾ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð¶Ð¸ Ð²ÐµÑÑŒ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€"})
    try:
        resp = client.chat.completions.create(model=model, messages=prompt[:10])
        summary = resp.choices[0].message.content
        new_prompt = sys_mess + [{"role": "system", "content": f"ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¹ Ð±ÐµÑÐµÐ´Ñ‹: {summary}"}]
        with open(filename, "w", encoding="utf-8") as f:
            json.dump({"messages": new_prompt}, f, ensure_ascii=False, indent=4)
    except OpenAIError as e:
        logging.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}")

async def start_and_check(event: NewMessage, message: str, chat_id: int) -> Tuple[str, Prompt]:
    file_num, filename, prompt = await read_existing_conversation(chat_id)
    prompt.append({"role": "user", "content": message})
    
    while True:
        tokens = num_tokens_from_messages(prompt, model)  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ð½Ð¾Ð²Ð»Ñ‘Ð½Ð½Ñ‹Ð¹ ÑÑ‡Ñ‘Ñ‚Ñ‡Ð¸Ðº
        if tokens > max_token - 1000:
            file_num += 1
            with open(f"{LOG_PATH}chats/session/{chat_id}.json", "w") as f:
                json.dump({"session": file_num}, f)
            await over_token(tokens, event, prompt, filename)
            _, filename, prompt = await read_existing_conversation(chat_id)
            prompt.append({"role": "user", "content": message})
        else:
            break
    return filename, prompt

def get_openai_response(prompt: Prompt, filename: str) -> str:
    try:
        response = client.chat.completions.create(
            model=model,  # Ð¢Ð¾Ñ‡Ð½Ð¾ o4-mini
            messages=prompt,
            max_tokens=1500,
            temperature=0.8,
        )
        text = response.choices[0].message.content.strip()
        prompt.append({"role": "assistant", "content": text})
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»Ñ‘Ð½Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° usage Ð´Ð»Ñ o4-mini (Ñ reasoning_tokens, Ð´ÐµÐºÐ°Ð±Ñ€ÑŒ 2025)
        used = response.usage.total_tokens if response.usage else 0
        reasoning_used = response.usage.completion_tokens_details.reasoning_tokens if hasattr(response.usage, 'completion_tokens_details') else 0
        left = max_token - used - reasoning_used
        
        with open(filename, "w", encoding="utf-8") as f:
            json.dump({"messages": prompt}, f, ensure_ascii=False, indent=4)
        
        return f"{text}\n\n__({left} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ, Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ reasoning: {reasoning_used})__"
        
    except OpenAIError as e:
        logging.error(f"OpenAI error (o4-mini): {e}")
        return "ÐžÐ¹, OpenAI ÑÐµÐ¹Ñ‡Ð°Ñ Ð¿Ð¾Ð´Ñ‚Ð¾Ñ€Ð¼Ð°Ð¶Ð¸Ð²Ð°ÐµÑ‚... ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð· Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ ðŸ˜"

# ÐžÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ (Gemini, Bing) â€” Ð¾ÑÑ‚Ð°Ð²ÑŒ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ, ÐµÑÐ»Ð¸ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑˆÑŒ, Ð¸Ð»Ð¸ ÑƒÐ´Ð°Ð»Ð¸ Ð´Ð»Ñ Ñ‡Ð¸ÑÑ‚Ð¾Ñ‚Ñ‹
async def process_and_send_mess(event, text: str, limit=500) -> None:
    text_lst = text.split("```")
    cur_limit = 4096
    for idx, part in enumerate(text_lst):
        if idx % 2 == 0:
            mess_gen = split_text(part, cur_limit)
            for mess in mess_gen:
                await event.client.send_message(
                    event.chat_id, mess, background=True, silent=True
                )
                await asyncio.sleep(1)
        else:
            mess_gen = split_text(part, cur_limit, prefix="```\n", suffix="\n```")
            for mess in mess_gen:
                await event.client.send_message(
                    event.chat_id, mess, background=True, silent=True
                )
                await asyncio.sleep(1)