import asyncio
import json
import logging
from typing import List, Tuple

import openai
from telethon.events import NewMessage

from src.utils import LOG_PATH

# ---- CHAT SETTINGS ----
model = "gpt-4o-mini"
max_token = 16000
sys_mess = "Ð¢Ñ‹ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð¸ ÐºÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚."


Prompt = List[dict]

async def over_token(num_tokens: int, event: NewMessage, prompt: Prompt, filename: str):
    await event.reply(f"Ð Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¹ ({num_tokens} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²), Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽ Ð½Ð¾Ð²Ñ‹Ð¹! ðŸ˜…")
    prompt.append({"role": "user", "content": "ÐšÑ€Ð°Ñ‚ÐºÐ¾ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð¶Ð¸ Ð²ÐµÑÑŒ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€"})
    try:
        completion = openai.ChatCompletion.create(model=model, messages=prompt[:10])
        summary = completion.choices[0].message.content
        new_prompt = sys_mess + [{"role": "system", "content": f"ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¹ Ð±ÐµÑÐµÐ´Ñ‹: {summary}"}]
        with open(filename, "w", encoding="utf-8") as f:
            json.dump({"messages": new_prompt}, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logging.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}")

async def start_and_check(event: NewMessage, message: str, chat_id: int) -> Tuple[str, Prompt]:
    file_num, filename, prompt = await read_existing_conversation(chat_id)
    prompt.append({"role": "user", "content": message})
    
    while True:
        tokens = num_tokens_from_messages(prompt, model)
        if tokens > max_token - 1000:
            file_num += 1
            with open(f"{LOG_PATH}chats/session/{chat_id}.json", "w") as f:
                json.dump({"session": file_num}, f)
            await over_token(tokens, event, prompt, filename)
            _, filename, prompt = await read_existing_conversation(chat_id)
            prompt.append({"role": "user", "content": message})
        else:
            break
    return filename, prompt

def get_openai_response(prompt: Prompt, filename: str) -> str:
    trial = 0

    while trial < 5:
        try:
            completion = openai.ChatCompletion.create(
                model=model,
                messages=prompt,
                max_tokens=1500,
                temperature=0.8,
            )

            text = completion.choices[0].message.content.strip()
            prompt.append(completion.choices[0].message)

            with open(filename, "w", encoding="utf-8") as f:
                json.dump({"messages": prompt}, f, ensure_ascii=False, indent=4)

            used = completion.usage.total_tokens
            left = max_token - used

            return f"{text}\n\n__({left} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ)__"

        except Exception as e:
            trial += 1
            logging.error(f"OpenAI error ({trial}/5): {e}")

            if trial >= 5:
                return "âš  OpenAI ÑÐµÐ¹Ñ‡Ð°Ñ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð· Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ."

            time.sleep(2)


async def process_and_send_mess(event, text: str, limit=500) -> None:
    from src.utils import split_text
    text_lst = text.split("```")
    cur_limit = 4096
    for idx, part in enumerate(text_lst):
        if idx % 2 == 0:
            mess_gen = split_text(part, cur_limit)
            for mess in mess_gen:
                await event.client.send_message(
                    event.chat_id, mess, background=True, silent=True
                )
                await asyncio.sleep(1)
        else:
            mess_gen = split_text(part, cur_limit, prefix="```\n", suffix="\n```")
            for mess in mess_gen:
                await event.client.send_message(
                    event.chat_id, mess, background=True, silent=True
                )
                await asyncio.sleep(1)