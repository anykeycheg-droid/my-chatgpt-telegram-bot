import asyncio
import json
import logging
import os  # â† Ð­Ð¢ÐžÐ™ Ð¡Ð¢Ð ÐžÐšÐ˜ ÐÐ• Ð‘Ð«Ð›Ðž â€” Ð¢Ð•ÐŸÐ•Ð Ð¬ Ð•Ð¡Ð¢Ð¬
from typing import List, Tuple

from openai import OpenAI
from openai import APIError
from telethon.events import NewMessage

from src.utils import LOG_PATH, model, max_token, sys_mess, read_existing_conversation, num_tokens_from_messages

Prompt = List[dict]

# ÐšÐ»Ð¸ÐµÐ½Ñ‚ OpenAI â€” Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð½Ð° Ð»ÑŽÐ±Ð¾Ð¹ Ð²ÐµÑ€ÑÐ¸Ð¸ (0.28 Ð¸ 1.x+)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

async def over_token(num_tokens: int, event: NewMessage, prompt: Prompt, filename: str):
    await event.reply(f"Ð Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ð¹ ({num_tokens} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²), Ð½Ð°Ñ‡Ð¸Ð½Ð°ÑŽ Ð½Ð¾Ð²Ñ‹Ð¹! ðŸ˜…")
    prompt.append({"role": "user", "content": "ÐšÑ€Ð°Ñ‚ÐºÐ¾ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð¶Ð¸ Ð²ÐµÑÑŒ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ð¹ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€"})
    try:
        resp = client.chat.completions.create(model=model, messages=prompt[:10])
        summary = resp.choices[0].message.content
        new_prompt = sys_mess + [{"role": "system", "content": f"ÐšÑ€Ð°Ñ‚ÐºÐ¾Ðµ Ñ€ÐµÐ·ÑŽÐ¼Ðµ Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¹ Ð±ÐµÑÐµÐ´Ñ‹: {summary}"}]
        with open(filename, "w", encoding="utf-8") as f:
            json.dump({"messages": new_prompt}, f, ensure_ascii=False, indent=4)
    except Exception as e:
        logging.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÑƒÐ¼Ð¼Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {e}")

async def start_and_check(event: NewMessage, message: str, chat_id: int) -> Tuple[str, Prompt]:
    file_num, filename, prompt = await read_existing_conversation(chat_id)
    prompt.append({"role": "user", "content": message})
    
    while True:
        tokens = num_tokens_from_messages(prompt, model)
        if tokens > max_token - 1000:
            file_num += 1
            with open(f"{LOG_PATH}chats/session/{chat_id}.json", "w") as f:
                json.dump({"session": file_num}, f)
            await over_token(tokens, event, prompt, filename)
            _, filename, prompt = await read_existing_conversation(chat_id)
            prompt.append({"role": "user", "content": message})
        else:
            break
    return filename, prompt

def get_openai_response(prompt: Prompt, filename: str) -> str:
    trial = 0
    while trial < 5:
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=prompt,
                max_completion_tokens=1500,
            )
            text = resp.choices[0].message.content.strip()
            prompt.append({"role": "assistant", "content": text})
            
            with open(filename, "w", encoding="utf-8") as f:
                json.dump({"messages": prompt}, f, ensure_ascii=False, indent=4)
            
            used = resp.usage.total_tokens if resp.usage else 0
            left = max_token - used
            return f"{text}\n\n__({left} Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð¾ÑÑ‚Ð°Ð»Ð¾ÑÑŒ)__"
            
        except Exception as e:
            logging.error(f"OpenAI error: {e}")
            trial += 1
            if trial >= 5:
                return "ÐžÐ¹, OpenAI ÑÐµÐ¹Ñ‡Ð°Ñ Ð¿Ð¾Ð´Ñ‚Ð¾Ñ€Ð¼Ð°Ð¶Ð¸Ð²Ð°ÐµÑ‚... ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹ ÐµÑ‰Ñ‘ Ñ€Ð°Ð· Ñ‡ÐµÑ€ÐµÐ· Ð¼Ð¸Ð½ÑƒÑ‚Ñƒ ðŸ˜"

async def process_and_send_mess(event, text: str, limit=500) -> None:
    from src.utils import split_text
    text_lst = text.split("```")
    cur_limit = 4096
    for idx, part in enumerate(text_lst):
        if idx % 2 == 0:
            mess_gen = split_text(part, cur_limit)
            for mess in mess_gen:
                await event.client.send_message(event.chat_id, mess, background=True, silent=True)
                await asyncio.sleep(1)
        else:
            mess_gen = split_text(part, cur_limit, prefix="```\n", suffix="\n```")
            for mess in mess_gen:
                await event.client.send_message(event.chat_id, mess, background=True, silent=True)
                await asyncio.sleep(1)