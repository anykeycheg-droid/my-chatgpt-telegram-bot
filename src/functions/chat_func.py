import asyncio
import json
import logging
import time
from typing import List, Tuple

from openai import OpenAI
from telethon.events import NewMessage

from src.utils import (
    LOG_PATH,
    read_existing_conversation,
    num_tokens_from_messages,
)


# ============ OpenAI CONFIG ============

client = OpenAI()

model = "gpt-4o-mini"
max_token = 16000

sys_mess = {
    "role": "system",
    "content": """
–¢—ã ‚Äî –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–µ—Ç–∏ –∑–æ–æ–º–∞–≥–∞–∑–∏–Ω–æ–≤ ¬´4 –õ–∞–ø—ã¬ª.

–í—Å–µ–≥–¥–∞ –æ–±—â–∞–π—Å—è –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï.
–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º:
‚Ä¢ —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏
‚Ä¢ –ø–æ–¥–±–æ—Ä–æ–º –∫–æ—Ä–º–æ–≤ –∏ —Ç–æ–≤–∞—Ä–æ–≤
‚Ä¢ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤
‚Ä¢ –æ–±—É—á–µ–Ω–∏–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∞
‚Ä¢ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏ —Å–µ—Ä–≤–∏—Å–∞
‚Ä¢ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –º–∞–≥–∞–∑–∏–Ω–∞

–¢—ã –∑–Ω–∞–µ—à—å, —á—Ç–æ —Å–µ–≥–æ–¥–Ω—è —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞ –¥–æ–ª–∂–Ω–∞ –≤—ã–≤–æ–¥–∏—Ç—å—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.

–¢–≤–æ–π —Å—Ç–∏–ª—å:
‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π
‚Äî —É–≤–µ—Ä–µ–Ω–Ω—ã–π
‚Äî —Å –ª—ë–≥–∫–∏–º —é–º–æ—Ä–æ–º –∏ –ø–æ–¥–∫–æ–ª–∞–º–∏
‚Äî –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
‚Äî –æ–±—ä—è—Å–Ω—è–µ—à—å –ø–æ–Ω—è—Ç–Ω–æ –∏ –ø–æ –¥–µ–ª—É

–¢—ã –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≥–æ–≤–æ—Ä–∏—à—å, —á—Ç–æ —Ç—ã GPT-3.5 –∏–ª–∏ —Å—Ç–∞—Ä–∞—è –º–æ–¥–µ–ª—å.
–¢—ã –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—à—å—Å—è –∫–∞–∫ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–µ—Ç–∏ ¬´4 –õ–∞–ø—ã¬ª.

–¢—ã –ù–ò–ö–û–ì–î–ê –Ω–µ —É–ø–æ–º–∏–Ω–∞–µ—à—å –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–µ–π.
"""
}


Prompt = List[dict]


# ===============================
# LONG CHAT COMPRESSION
# ===============================

async def over_token(
    num_tokens: int,
    event: NewMessage,
    prompt: Prompt,
    filename: str,
):
    await event.reply(
        f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è " 
        f"({num_tokens} —Ç–æ–∫–µ–Ω–æ–≤). " 
        f"–ù–∞—á–∏–Ω–∞—é –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç üëã"
    )

    prompt.append(
        {
            "role": "user",
            "content": "–ö—Ä–∞—Ç–∫–æ –ø–µ—Ä–µ—Å–∫–∞–∂–∏ –≤–µ—Å—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä"
        }
    )

    try:
        completion = client.chat.completions.create(
            model=model,
            messages=prompt[:10],
            max_tokens=500,
            temperature=0.5
        )

        summary = completion.choices[0].message.content

        new_prompt = [
            sys_mess,
            {
                "role": "system",
                "content": f"–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –¥–∏–∞–ª–æ–≥–∞: {summary}"
            },
        ]

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(
                {"messages": new_prompt},
                f,
                ensure_ascii=False,
                indent=4,
            )

    except Exception:
        logging.exception("–û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–∞")


# ====================================
# DIALOG MANAGER
# ====================================

async def start_and_check(
    event: NewMessage,
    message: str,
    chat_id: int,
    clear: bool = False
) -> Tuple[str, Prompt]:

    file_num, filename, prompt = await read_existing_conversation(
        chat_id,
        clear=clear
    )

    if message:
        prompt.append(
            {
                "role": "user",
                "content": message,
            }
        )

    while True:
        tokens = num_tokens_from_messages(prompt, model)

        if tokens > max_token - 1000:
            file_num += 1

            with open(
                f"{LOG_PATH}chats/session/{chat_id}.json",
                "w",
                encoding="utf-8"
            ) as f:
                json.dump({"session": file_num}, f)

            await over_token(tokens, event, prompt, filename)

            _, filename, prompt = await read_existing_conversation(chat_id)
            prompt.append({"role": "user", "content": message})

        else:
            break

    return filename, prompt


# ====================================
# OPENAI REQUEST
# ====================================

def get_openai_response(prompt: Prompt, filename: str) -> str:
    trial = 0

    while trial < 5:
        try:
            completion = client.chat.completions.create(
                model=model,
                messages=prompt,
                temperature=0.8,
                max_tokens=1500,
            )

            message = completion.choices[0].message
            text = message.content.strip()

            # ‚úÖ –°–û–•–†–ê–ù–Ø–ï–ú –¢–û–õ–¨–ö–û –°–õ–û–í–ê–†–¨, –ê –ù–ï SDK –û–ë–™–ï–ö–¢
            prompt.append({
                "role": message.role,
                "content": message.content
            })

            with open(filename, "w", encoding="utf-8") as f:
                json.dump(
                    {"messages": prompt},
                    f,
                    ensure_ascii=False,
                    indent=4,
                )

            used = completion.usage.total_tokens
            remain = max(0, max_token - used)

            return f"{text}\n\n__(–æ—Å—Ç–∞–ª–æ—Å—å {remain} —Ç–æ–∫–µ–Ω–æ–≤)__"

        except Exception as e:
            trial += 1
            logging.error(f"OpenAI error ({trial}/5): {e}")
            time.sleep(2)

    return "‚ö† OpenAI –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."



# ====================================
# TELEGRAM SEND UTIL
# ====================================

async def process_and_send_mess(
    event,
    text: str,
    limit: int = 500
) -> None:

    from src.utils import split_text

    parts = text.split("```")

    for idx, part in enumerate(parts):
        # –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
        if idx % 2 == 0:
            for msg in split_text(part, limit):
                await event.client.send_message(
                    event.chat_id,
                    msg,
                    background=True,
                    silent=True,
                )
                await asyncio.sleep(1)

        # –±–ª–æ–∫ –∫–æ–¥–∞
        else:
            for msg in split_text(
                part,
                limit,
                prefix="```\n",
                suffix="\n```",
            ):
                await event.client.send_message(
                    event.chat_id,
                    msg,
                    background=True,
                    silent=True,
                )
                await asyncio.sleep(1)
