import json
import logging
import os
import base64
import time
from typing import List, Tuple, Optional

from openai import OpenAI
from telethon.events import NewMessage

from src.utils import (
    LOG_PATH,
    model,
    max_token,
    sys_mess,
    read_existing_conversation,
    num_tokens_from_messages,
)

# Init OpenAI client
client = OpenAI()

Prompt = List[dict]


# ==============================
# IMAGE ANALYSIS
# ==============================

async def analyze_image_with_gpt(
    image_path: str,
    user_prompt: Optional[str] = None
) -> str:
    """
    –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ GPT.
    –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º —Å handlers.py
    """

    try:
        with open(image_path, "rb") as f:
            image_data = base64.b64encode(
                f.read()
            ).decode("utf-8")

        if not user_prompt:
            user_prompt = "–ß—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏?"

        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": user_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            }
                        },
                    ],
                }
            ],
            max_tokens=max_token,
            temperature=0.2,
        )

        return response.choices[0].message.content.strip()

    except Exception:
        logging.exception("–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        return "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."


# ==============================
# CHAT FLOW
# ==============================

async def start_and_check(
    event: NewMessage,
    message: str,
    chat_id: int,
) -> Tuple[str, Prompt]:
    file_num, filename, prompt = await read_existing_conversation(chat_id)

    prompt.append(
        {"role": "user", "content": message}
    )

    while True:
        tokens = num_tokens_from_messages(prompt, model)

        if tokens > max_token - 1000:
            file_num += 1

            with open(f"{LOG_PATH}/chats/session/{chat_id}.json", "w") as f:
                json.dump({"session": file_num}, f)

            await over_token(tokens, event, prompt, filename)
            _, filename, prompt = await read_existing_conversation(chat_id)

            prompt.append(
                {"role": "user", "content": message}
            )

        else:
            break

    return filename, prompt


# ==============================
# TOKEN OVERFLOW
# ==============================

async def over_token(
    num_tokens: int,
    event: NewMessage,
    prompt: Prompt,
    filename: str,
):
    try:
        await event.reply(
            f"–î–∏–∞–ª–æ–≥ —Å—Ç–∞–ª —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º ({num_tokens} —Ç–æ–∫–µ–Ω–æ–≤). –ù–∞—á–∏–Ω–∞—é –Ω–æ–≤—ã–π —á–∞—Ç üôÇ"
        )

        completion = client.chat.completions.create(
            model=model,
            messages=prompt,
            max_tokens=300,
            temperature=0.2,
        )

        summary = completion.choices[0].message.content

        new_prompt = [
            {
                "role": "system",
                "content": f"–†–µ–∑—é–º–µ –ø—Ä–æ—à–ª–æ–≥–æ –¥–∏–∞–ª–æ–≥–∞: {summary}",
            }
        ]

        with open(filename, "w", encoding="utf-8") as f:
            json.dump({"messages": new_prompt}, f, ensure_ascii=False, indent=2)

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–µ–∑–∫–∏ –¥–∏–∞–ª–æ–≥–∞: {e}")


# ==============================
# OPENAI RESPONSE
# ==============================

def get_openai_response(prompt: Prompt, filename: str) -> str:

    for attempt in range(1, 6):
        try:
            completion = client.chat.completions.create(
                model=model,
                messages=prompt,
                max_tokens=1500,
                temperature=0.6,
            )

            text = completion.choices[0].message.content.strip()

            prompt.append(completion.choices[0].message)

            with open(filename, "w", encoding="utf-8") as f:
                json.dump({"messages": prompt}, f, ensure_ascii=False, indent=2)

            used = completion.usage.total_tokens
            remain = max(0, max_token - used)

            return f"{text}\n\n_(–æ—Å—Ç–∞–ª–æ—Å—å {remain} —Ç–æ–∫–µ–Ω–æ–≤)_"

        except Exception as e:
            logging.error(f"OpenAI error ({attempt}/5): {e}")
            time.sleep(2)

    return "‚ö†Ô∏è OpenAI –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."


# ==============================
# TELEGRAM OUTPUT
# ==============================

async def process_and_send_mess(event, answer: str):

    max_length = 4000

    for i in range(0, len(answer), max_length):
        await event.reply(answer[i:i + max_length])
