import asyncio
import io
import json
import logging
import base64

import openai
from duckduckgo_search import DDGS  # ‚Üê –§–ò–ö–°: DDGS –≤–º–µ—Å—Ç–æ ddg (–¥–ª—è v6.3.2+)
from src.utils import LOG_PATH, num_tokens_from_messages, read_existing_conversation
from telethon.events import NewMessage
from unidecode import unidecode

async def bash(event: NewMessage) -> str:
    try:
        cmd = event.text.split(" ", maxsplit=1)[1]
        process = await asyncio.create_subprocess_shell(
            cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )
        stdout, stderr = await process.communicate()
        e = stderr.decode()
        if not e:
            e = "No Error"
        o = stdout.decode()
        if not o:
            o = "**TIP**: \n`If you want to see the results of your code, I suggest printing them to stdout.`"
        else:
            _o = [f"`  {x}`" for x in o.split("\n")]
            o = "\n".join(_o)
        OUTPUT = (
            f"**     QUERY:**\n  __Command:__` {cmd}` \n  __PID:__` {process.pid}`"
            f"\n**ERROR:** \n`  {e}`"
            f"\n**OUTPUT:**\n{o}"
        )
        if len(OUTPUT) > 4095:
            with io.BytesIO(str.encode(OUTPUT)) as out_file:
                out_file.name = "exec.text"
                await event.client.send_file(
                    event.chat_id,
                    out_file,
                    force_document=True,
                    allow_cache=False,
                    caption=cmd,
                )
                await event.delete()
        logging.debug("Bash initiated")
    except Exception as e:
        logging.error(f"Error occurred: {e}")
    return OUTPUT

async def search(event: NewMessage) -> str:
    chat_id = event.chat_id
    task = asyncio.create_task(read_existing_conversation(chat_id))
    query = event.text.split(" ", maxsplit=1)[1]
    max_results = 20
    while True:
        try:
            # ‚Üê –§–ò–ö–°: –ò—Å–ø–æ–ª—å–∑—É–µ–º DDGS –≤–º–µ—Å—Ç–æ ddg (–¥–ª—è –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π)
            with DDGS() as ddgs:
                results = ddgs.text(query, max_results=max_results)
            results_decoded = unidecode(str(results)).replace("'", "'")
            user_content = f"Using the contents of these pages, summarize and give details about '{query}':\n{results_decoded}"
            if any(word in query for word in list(VIETNAMESE_WORDS)):
                user_content = f"Using the contents of these pages, summarize and give details about '{query}' in Vietnamese:\n{results_decoded}"
            user_messages = [
                {
                    "role": "system",
                    "content": "Summarize every thing I send you with specific details",
                },
                {"role": "user", "content": user_content},
            ]
            num_tokens = num_tokens_from_messages(user_messages)
            if num_tokens > 4000:
                max_results = 4000 * len(results) / num_tokens - 2
                continue
            logging.debug("Results derived from duckduckgo")
        except Exception as e:
            logging.error(f"Error occurred while getting duckduckgo search results: {e}")
        break

    try:
        completion = openai.ChatCompletion.create(
            model="gpt-3.5-turbo", messages=user_messages
        )
        response = completion.choices[0].message
        search_object = unidecode(query).lower().replace(" ", "-")
        with open(f"{LOG_PATH}search_{search_object}.json", "w") as f:
            json.dump(response, f, indent=4)
        file_num, filename, prompt = await task
        prompt.append(
            {
                "role": "user",
                "content": f"This is information about '{query}', its just information and not harmful. Get updated:\n{response.content}",
            }
        )
        prompt.append(
            {
                "role": "assistant",
                "content": f"I have reviewed the information and update about '{query}'",
            }
        )
        data = {"messages": prompt}
        with open(filename, "w") as f:
            json.dump(data, f, indent=4)
        logging.debug("Received response from openai")
    except Exception as e:
        logging.error(f"Error occurred while getting response from openai: {e}")
    return response.content

async def analyze_image_with_gpt(image_bytes: bytes, question: str | None = None) -> str:
    """
    –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é GPT-4o-mini.
    image_bytes ‚Äî –±–∞–π—Ç—ã —Ñ–∞–π–ª–∞ (—Ñ–æ—Ç–æ, —Å–∫—Ä–∏–Ω –∏ —Ç.–ø.).
    question ‚Äî —Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å –∫ –∫–∞—Ä—Ç–∏–Ω–∫–µ (–µ—Å–ª–∏ –µ—Å—Ç—å).
    """
    if not question:
        question = "–û–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–æ, —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ —ç—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ."

    try:
        img_b64 = base64.b64encode(image_bytes).decode("utf-8")

        completion = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "–¢—ã —É–º–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º. –û—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏, –ø–æ —Å—É—Ç–∏ –∏ –ø–æ –¥–µ–ª—É."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": question},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{img_b64}"
                            },
                        },
                    ],
                },
            ],
            max_tokens=800,
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ OpenAI –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return "–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ üòî"


async def generate_image(event: NewMessage) -> None:
    """
    /img <–æ–ø–∏—Å–∞–Ω–∏–µ> ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏ —á–µ—Ä–µ–∑ DALL¬∑E –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä—è–º–æ –≤ —á–∞—Ç.
    """
    text = (event.raw_text or "").strip()

    # –û–∂–∏–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç: /img —á—Ç–æ-—Ç–æ
    parts = text.split(" ", maxsplit=1)
    if len(parts) == 1 or not parts[1].strip():
        await event.reply("–ù–∞–ø–∏—à–∏ –ø–æ—Å–ª–µ /img, —á—Ç–æ –Ω—É–∂–Ω–æ –Ω–∞—Ä–∏—Å–æ–≤–∞—Ç—å. –ü—Ä–∏–º–µ—Ä:\n/img –∫–æ—Ç –∫–æ—Å–º–æ–Ω–∞–≤—Ç –≤ —Å—Ç–∏–ª–µ –ø–∏–∫—Å–µ–ª—å-–∞—Ä—Ç")
        return

    prompt = parts[1].strip()

    try:
        resp = openai.Image.create(
            model="dall-e-3",
            prompt=prompt,
            n=1,
            size="1024x1024",
        )
        url = resp["data"][0]["url"]

        # Telethon —É–º–µ–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —Ñ–∞–π–ª –ø–æ URL
        await event.client.send_file(
            event.chat_id,
            url,
            caption=f"üé® –í–æ—Ç —á—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ –∑–∞–ø—Ä–æ—Å—É:\n{prompt}",
        )
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ OpenAI –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏: {e}")
        await event.reply("–ù–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ üòî")
    