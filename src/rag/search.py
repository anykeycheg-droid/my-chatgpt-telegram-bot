import json
import logging
from pathlib import Path
from typing import List, Dict, Any

import faiss
from sentence_transformers import SentenceTransformer

BASE_DIR = Path(__file__).resolve().parent
INDEX_FILE = BASE_DIR / "faiss.index"
DOCS_FILE = BASE_DIR / "docs.json"

# –û–≥—Ä–∞–Ω–∏—á–∏–º —á–∏—Å–ª–æ –ø–æ—Ç–æ–∫–æ–≤ FAISS (—á—É—Ç—å —ç–∫–æ–Ω–æ–º–∏–º –ø–∞–º—è—Ç—å –∏ CPU)
try:
    faiss.omp_set_num_threads(1)
except Exception:
    # –µ—Å–ª–∏ —Å–±–æ—Ä–∫–∞ –±–µ–∑ OpenMP ‚Äî –ø—Ä–æ—Å—Ç–æ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
    pass

MODEL = SentenceTransformer("all-MiniLM-L6-v2")

INDEX = None
CHUNKS: List[Dict[str, Any]] = []
RAG_READY = False


def _load_index_mmap() -> None:
    """–ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ –≤ —Ä–µ–∂–∏–º–µ memory-mapped.

    –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ ‚Äî –ø—Ä–æ—Å—Ç–æ –æ—Ç–∫–ª—é—á–∞–µ–º RAG
    –∏ –¥–∞—ë–º –±–æ—Ç—É —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞.
    """
    global INDEX, CHUNKS, RAG_READY

    try:
        if not INDEX_FILE.exists():
            logging.error(f"FAISS index file not found: {INDEX_FILE}")
            RAG_READY = False
            return

        if not DOCS_FILE.exists():
            logging.error(f"Docs file not found: {DOCS_FILE}")
            RAG_READY = False
            return

        logging.info(f"Loading FAISS index (mmap) from {INDEX_FILE}")

        # üî• –ö–ª—é—á–µ–≤–æ–π –º–æ–º–µ–Ω—Ç: –∏—Å–ø–æ–ª—å–∑—É–µ–º memory-mapped —Ä–µ–∂–∏–º
        INDEX = faiss.read_index(str(INDEX_FILE), faiss.IO_FLAG_MMAP)

        with open(DOCS_FILE, encoding="utf-8") as f:
            CHUNKS = json.load(f)

        RAG_READY = True
        logging.info(
            "FAISS index loaded in mmap mode. "
            f"Chunks: {len(CHUNKS)}"
        )

    except Exception:
        logging.exception("Failed to load FAISS index in mmap mode")
        INDEX = None
        CHUNKS = []
        RAG_READY = False


# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
_load_index_mmap()


def search(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    FAISS-–ø–æ–∏—Å–∫ –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –±–∞–∑–µ.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π:
    {
        "rank": int,
        "score": float,
        "text": str,
        "source": str,
        "source_file": str,
        "page": int | None,
        "section": str | None,
    }
    """
    if not query:
        return []

    if not RAG_READY or INDEX is None or not CHUNKS:
        # –ò–Ω–¥–µ–∫—Å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è –∏–ª–∏ –±–∞–∑–∞ –ø—É—Å—Ç–∞ ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫.
        # –í–Ω–µ—à–Ω—è—è –ª–æ–≥–∏–∫–∞ (chat_func.try_rag) –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ç–æ.
        logging.warning("RAG search requested, but index is not ready")
        return []

    # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
    v = MODEL.encode([query])

    distances, indices = INDEX.search(v, top_k)

    results: List[Dict[str, Any]] = []
    for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), start=1):
        if idx < 0:
            continue
        if idx >= len(CHUNKS):
            continue

        chunk = CHUNKS[idx]

        if isinstance(chunk, dict):
            text = chunk.get("text")
            source = chunk.get("source")
            source_file = chunk.get("source_file")
            page = chunk.get("page")
            section = chunk.get("section")
        else:
            # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç docs.json
            text = str(chunk)
            source = None
            source_file = None
            page = None
            section = None

        results.append(
            {
                "rank": rank,
                "score": float(dist),
                "text": text,
                "source": source,
                "source_file": source_file,
                "page": page,
                "section": section,
            }
        )

    return results
